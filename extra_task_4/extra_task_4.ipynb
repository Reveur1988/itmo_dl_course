{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a38cfe7e-c4f1-4fc5-8d32-16136878b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1162c405-cabc-43a0-b3b8-a94901b0e657",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2c490f-2c2f-4111-b1f6-cd2f82241925",
   "metadata": {},
   "source": [
    " ## 1. –í—ã–±–æ—Ä –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    " –î–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤ –≤—ã–±—Ä–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç IMDB Reviews.\n",
    " –≠—Ç–æ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—Ç–∑—ã–≤—ã –æ —Ñ–∏–ª—å–º–∞—Ö —Å –±–∏–Ω–∞—Ä–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π sentiment (positive/negative).\n",
    "\n",
    " –í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–º–µ–Ω—å—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ (4000 –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ 1000 –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è) –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö. –ü—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –±–æ–ª–µ–µ –º–æ—â–Ω–æ–≥–æ GPU –∏–ª–∏ –±–æ–ª—å—à–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –±—ã–ª–æ –±—ã —Ü–µ–ª–µ—Å–æ–æ–±—Ä–∞–∑–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "512c8be5-72df-43c6-80bd-284ff59a1539",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(4000))\n",
    "test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "dataset_sampled = {\"train\": train_dataset, \"test\": test_dataset}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff57538f-602b-46c7-b759-9bc3b9efd077",
   "metadata": {},
   "source": [
    " ## 2. –í—ã–±–æ—Ä –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏\n",
    "\n",
    " –í –∫–∞—á–µ—Å—Ç–≤–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –≤—ã–±—Ä–∞–Ω DistilBERT - –æ–±–ª–µ–≥—á–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è BERT,\n",
    " –∫–æ—Ç–æ—Ä–∞—è —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç 97% –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ 60% –±—ã—Å—Ç—Ä–µ–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad5828d-0916-4458-94ab-c08c58c38a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98179930-f80d-45dd-a4f3-cbd8fb666185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020ef38cf62e40f0ad5504a78f7f3962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da8a4fb294c436bb24b3f74992f7c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = {}\n",
    "for split in dataset_sampled:\n",
    "    tokenized_datasets[split] = dataset_sampled[split].map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\n",
    "            col for col in dataset_sampled[split].column_names if col not in [\"label\"]\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a38f4c5-98aa-480b-8175-027460f42492",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd43b1d-5fa5-466f-add5-582c39eadfbf",
   "metadata": {},
   "source": [
    " ## 3. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "\n",
    " –î–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è accuracy –∏ F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a4b0284-2029-4658-a581-fcfdfaf60f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1_score = evaluate.load(\"f1\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=predictions, references=labels)[\n",
    "            \"accuracy\"\n",
    "        ],\n",
    "        \"f1\": f1_score.compute(\n",
    "            predictions=predictions, references=labels, average=\"weighted\"\n",
    "        )[\"f1\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc4c39c1-fde8-425c-95cc-0cd42cca578e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0552b622-39dd-4f62-8e2e-d24c846b1367",
   "metadata": {},
   "source": [
    " ## 4. –û—Ü–µ–Ω–∫–∞ –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
    " –ò–∑–º–µ—Ä–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ DistilBERT –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ IMDB Reviews –¥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2d0b342-b827-4b0b-9e4a-f1d5114773ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64956/2559754658.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial metrics: {'eval_loss': 0.6967155933380127, 'eval_model_preparation_time': 0.001, 'eval_accuracy': 0.408, 'eval_f1': 0.404652143178459, 'eval_runtime': 10.3939, 'eval_samples_per_second': 96.211, 'eval_steps_per_second': 12.026}\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "initial_metrics = trainer.evaluate()\n",
    "print(\"Initial metrics:\", initial_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0ad86a-12e9-4335-9083-fee44dd40098",
   "metadata": {},
   "source": [
    " ## 5. –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "\n",
    " –ü—Ä–æ—Ü–µ—Å—Å –¥–æ–æ–±—É—á–µ–Ω–∏—è (fine-tuning) –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è –Ω–∞ –≤—ã–±–æ—Ä–∫–µ –∏–∑ 4000 —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    " –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ IMDB. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    " - –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (learning rate): 2e-5\n",
    " - –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: 32\n",
    " - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö: 2\n",
    " - L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (weight decay): 0.01\n",
    "\n",
    " –ù–∞ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbec3432-e95d-47cb-b391-8e554778e024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reveur/anaconda3/envs/itmo_dl_course/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_64956/2691310023.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 10:36, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.369700</td>\n",
       "      <td>0.329480</td>\n",
       "      <td>0.858000</td>\n",
       "      <td>0.857508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.212800</td>\n",
       "      <td>0.292642</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.871008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=0.3525660514831543, metrics={'train_runtime': 636.8411, 'train_samples_per_second': 12.562, 'train_steps_per_second': 0.393, 'total_flos': 529869594624000.0, 'train_loss': 0.3525660514831543, 'epoch': 2.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"distilbert-imdb-classifier\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cbf54f4-c8de-4caf-a92d-61ba908d0dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of metrics:\n",
      "Before fine-tuning: {'eval_loss': 0.6967155933380127, 'eval_model_preparation_time': 0.001, 'eval_accuracy': 0.408, 'eval_f1': 0.404652143178459, 'eval_runtime': 10.3939, 'eval_samples_per_second': 96.211, 'eval_steps_per_second': 12.026}\n",
      "After fine-tuning: {'eval_loss': 0.29264160990715027, 'eval_accuracy': 0.871, 'eval_f1': 0.8710081270731437, 'eval_runtime': 24.1998, 'eval_samples_per_second': 41.323, 'eval_steps_per_second': 1.322, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "final_metrics = trainer.evaluate()\n",
    "print(\"\\nComparison of metrics:\")\n",
    "print(\"Before fine-tuning:\", initial_metrics)\n",
    "print(\"After fine-tuning:\", final_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c54f82-c609-4e9e-bbc8-dd7c2b8b55c0",
   "metadata": {},
   "source": [
    " ## 6. –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "\n",
    " ### –ú–µ—Ç—Ä–∏–∫–∏ –¥–æ fine-tuning:\n",
    " - Accuracy: 0.408 (40.8%)\n",
    " - F1-score: 0.405 (40.5%)\n",
    "\n",
    " ### –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ—Å–ª–µ fine-tuning:\n",
    " - Accuracy: 0.871 (87.1%)\n",
    " - F1-score: 0.871 (87.1%)\n",
    "\n",
    " ### –í—ã–≤–æ–¥—ã:\n",
    " 1. –ò—Å—Ö–æ–¥–Ω–∞—è –º–æ–¥–µ–ª—å –ø–æ–∫–∞–∑–∞–ª–∞ –Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–æ–∫–æ–ª–æ 40%), —á—Ç–æ –æ–∂–∏–¥–∞–µ–º–æ, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∞ –Ω–µ –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –¥–∞–Ω–Ω–æ–π –∑–∞–¥–∞—á–µ.\n",
    " 2. –ü–æ—Å–ª–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏–ª–æ—Å—å - –æ–±–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ 87%.\n",
    " 3. –†–∞–≤–µ–Ω—Å—Ç–≤–æ –º–µ—Ç—Ä–∏–∫ accuracy –∏ F1-score –≥–æ–≤–æ—Ä–∏—Ç –æ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ.\n",
    " 4. –î–æ—Å—Ç–∏–≥–Ω—É—Ç—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –º–æ–∂–Ω–æ —Å—á–∏—Ç–∞—Ç—å —É—Å–ø–µ—à–Ω—ã–º –¥–ª—è –∑–∞–¥–∞—á–∏ –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤.\n",
    "\n",
    " –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, fine-tuning –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ DistilBERT –ø–æ–∑–≤–æ–ª–∏–ª —É—Å–ø–µ—à–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –µ—ë –¥–ª—è —Ä–µ—à–µ–Ω–∏—è\n",
    " –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ—Ç–∑—ã–≤–æ–≤ IMDB."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itmo_dl_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
