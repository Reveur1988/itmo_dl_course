{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3149b848-cc29-4af6-bc19-5b7f54287a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from pathlib import Path\n",
    "\n",
    "import gensim.downloader as api\n",
    "import kagglehub\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, TQDMProgressBar\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics import F1Score, MetricCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "166aded9-9dd2-4259-9568-d513abb4e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27c3afc0-e1fc-4b12-82bd-20af4534038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = kagglehub.dataset_download(\"andrewmvd/trip-advisor-hotel-reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb7f17-f653-459a-a491-7dadef66ff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = list(Path(data_dir).glob(\"*.csv\"))\n",
    "\n",
    "if len(csv_files) == 1:\n",
    "    data_path = csv_files[0]\n",
    "    print(f\"Found CSV file: {data_path}\")\n",
    "elif len(csv_files) > 1:\n",
    "    file_names = [file.name for file in csv_files]\n",
    "    raise ValueError(f\"More than one CSV file found: {file_names}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No CSV files found in the directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35de642-7a14-4f66-b391-c08678c7cb1f",
   "metadata": {},
   "source": [
    "  # Выбор метрик оценки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd94b5-8ccb-47c3-8e19-317eb77ba8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path)\n",
    "print(df[\"Rating\"].value_counts().sort_index() * 100 / len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfa3828-cf8b-4930-889a-0c7c802c477c",
   "metadata": {},
   "source": [
    "  Мы видим сильный дисбаланс классов, поэтому будем использовать:\n",
    "  - F1-score с параметром average=\"macro\" - уравновешивает вклад всех классов, акцент на малочисленных классах\n",
    "  - F1-score с параметром average=\"weighted\" - учитывает дисбаланс классов, больше вес крупных классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f8196a-930f-415a-a0f5-d53ff8e0f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "resources = [\n",
    "    \"punkt\",\n",
    "    \"stopwords\",\n",
    "    \"wordnet\",\n",
    "    \"punkt_tab\",\n",
    "]\n",
    "\n",
    "for resource in resources:\n",
    "    nltk.download(resource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "becb33c4-491b-4dac-850c-58cc606619d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words(\"english\"))\n",
    "        self.punctuation = set(string.punctuation)\n",
    "        self.glove = api.load(\"glove-wiki-gigaword-100\")\n",
    "        self.embedding_dim = 100\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tokens = [\n",
    "            token\n",
    "            for token in tokens\n",
    "            if token not in self.punctuation\n",
    "            and token not in self.stop_words\n",
    "            and not token.isnumeric()\n",
    "        ]\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    def text_to_embedding(self, tokens, max_length=100):\n",
    "        embeddings = []\n",
    "        for token in tokens[:max_length]:\n",
    "            if token in self.glove:\n",
    "                embeddings.append(self.glove[token])\n",
    "            else:\n",
    "                embeddings.append(np.zeros(self.embedding_dim))\n",
    "\n",
    "        while len(embeddings) < max_length:\n",
    "            embeddings.append(np.zeros(self.embedding_dim))\n",
    "\n",
    "        return torch.FloatTensor(np.array(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d33e387-ffcf-485f-b22e-630eec5590a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HotelReviewsDataset(Dataset):\n",
    "    def __init__(self, texts, ratings):\n",
    "        self.preprocessor = TextPreprocessor()\n",
    "        self.texts = []\n",
    "\n",
    "        for text in texts:\n",
    "            tokens = self.preprocessor.preprocess_text(text)\n",
    "            embedding = self.preprocessor.text_to_embedding(tokens)\n",
    "            self.texts.append(embedding)\n",
    "\n",
    "        self.ratings = ratings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.ratings[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9b316b5-d584-4f33-ac4b-5090cc178b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HotelReviewsDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path: str, batch_size: int = 32):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = os.cpu_count()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        df = pd.read_csv(self.data_path)\n",
    "        self.df = df[[\"Review\", \"Rating\"]]\n",
    "\n",
    "        self.le = LabelEncoder()\n",
    "        self.df[\"Rating\"] = self.le.fit_transform(self.df[\"Rating\"])\n",
    "\n",
    "        self.train_val_df, self.test_df = train_test_split(\n",
    "            self.df, test_size=0.2, stratify=self.df[\"Rating\"], random_state=42\n",
    "        )\n",
    "\n",
    "        self.train_df, self.val_df = train_test_split(\n",
    "            self.train_val_df,\n",
    "            test_size=0.25,\n",
    "            stratify=self.train_val_df[\"Rating\"],\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        if stage == \"fit\":\n",
    "            self.train_dataset = HotelReviewsDataset(\n",
    "                self.train_df[\"Review\"].values,\n",
    "                torch.tensor(self.train_df[\"Rating\"].values),\n",
    "            )\n",
    "            self.val_dataset = HotelReviewsDataset(\n",
    "                self.val_df[\"Review\"].values, torch.tensor(self.val_df[\"Rating\"].values)\n",
    "            )\n",
    "\n",
    "        if stage == \"test\":\n",
    "            self.test_dataset = HotelReviewsDataset(\n",
    "                self.test_df[\"Review\"].values,\n",
    "                torch.tensor(self.test_df[\"Rating\"].values),\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e63b731-cdd5-4b25-bf24-6b03b4c77b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(pl.LightningModule):\n",
    "    def __init__(self, input_dim=100, num_classes=5, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(input_dim, 128, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.conv2 = nn.Conv1d(128, 256, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.conv3 = nn.Conv1d(256, 512, kernel_size=7, padding=3)\n",
    "        self.bn3 = nn.BatchNorm1d(512)\n",
    "\n",
    "        self.global_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "        self.metrics = MetricCollection(\n",
    "            {\n",
    "                \"f1_macro\": F1Score(\n",
    "                    task=\"multiclass\", num_classes=num_classes, average=\"macro\"\n",
    "                ),\n",
    "                \"f1_weighted\": F1Score(\n",
    "                    task=\"multiclass\", num_classes=num_classes, average=\"weighted\"\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        self.train_metrics = self.metrics.clone(prefix=\"train_\")\n",
    "        self.val_metrics = self.metrics.clone(prefix=\"val_\")\n",
    "        self.test_metrics = self.metrics.clone(prefix=\"test_\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.global_pool(x)\n",
    "        x = x.squeeze(-1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        self.train_metrics.update(logits, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.log_dict(self.train_metrics.compute(), prog_bar=True)\n",
    "        self.train_metrics.reset()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        self.val_metrics.update(logits, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.log_dict(self.val_metrics.compute(), prog_bar=True)\n",
    "        self.val_metrics.reset()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        self.test_metrics.update(logits, y)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.log_dict(self.test_metrics.compute(), prog_bar=True)\n",
    "        self.test_metrics.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(), lr=self.hparams.learning_rate, weight_decay=0.01\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.5, patience=3\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f0a44de-1a7b-4062-967e-99e7e03dc0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, input_dim=100, hidden_dim=256, num_classes=5, learning_rate=1e-3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "        self.metrics = MetricCollection(\n",
    "            {\n",
    "                \"f1_macro\": F1Score(\n",
    "                    task=\"multiclass\", num_classes=num_classes, average=\"macro\"\n",
    "                ),\n",
    "                \"f1_weighted\": F1Score(\n",
    "                    task=\"multiclass\", num_classes=num_classes, average=\"weighted\"\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        self.train_metrics = self.metrics.clone(prefix=\"train_\")\n",
    "        self.val_metrics = self.metrics.clone(prefix=\"val_\")\n",
    "        self.test_metrics = self.metrics.clone(prefix=\"test_\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        attention_output = torch.sum(attention_weights * lstm_out, dim=1)\n",
    "        x = self.dropout(attention_output)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        self.train_metrics.update(logits, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.log_dict(self.train_metrics.compute(), prog_bar=True)\n",
    "        self.train_metrics.reset()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        self.val_metrics.update(logits, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.log_dict(self.val_metrics.compute(), prog_bar=True)\n",
    "        self.val_metrics.reset()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        self.test_metrics.update(logits, y)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.log_dict(self.test_metrics.compute(), prog_bar=True)\n",
    "        self.test_metrics.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=1e-5,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.5, patience=3\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1710957-3c4c-4557-a89d-08be9ca5e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72aff0b5-be10-46a6-9da1-632c690a78c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = HotelReviewsDataModule(data_path)\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bf8882-4daa-4dbc-ba05-695c3f90a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNNModel()\n",
    "checkpoint_callback_cnn = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"cnn-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=1,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=25,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        TQDMProgressBar(refresh_rate=1),\n",
    "        EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3),\n",
    "        checkpoint_callback_cnn,\n",
    "    ],\n",
    "    logger=TensorBoardLogger(save_dir=\"lightning_logs\", name=\"hotel_reviews_cnn\"),\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True,\n",
    ")\n",
    "\n",
    "trainer.fit(model=cnn_model, datamodule=datamodule)\n",
    "best_cnn = CNNModel.load_from_checkpoint(checkpoint_callback_cnn.best_model_path)\n",
    "\n",
    "cnn_results = trainer.test(model=best_cnn, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be22c3-4b2b-4aa0-b043-f5afa591e208",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTMModel()\n",
    "checkpoint_callback_lstm = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"lstm-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=1,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=25,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        TQDMProgressBar(refresh_rate=1),\n",
    "        EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3),\n",
    "        checkpoint_callback_lstm,\n",
    "    ],\n",
    "    logger=TensorBoardLogger(save_dir=\"lightning_logs\", name=\"hotel_reviews_lstm\"),\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True,\n",
    ")\n",
    "\n",
    "trainer.fit(model=lstm_model, datamodule=datamodule)\n",
    "best_lstm = LSTMModel.load_from_checkpoint(checkpoint_callback_lstm.best_model_path)\n",
    "lstm_results = trainer.test(model=best_lstm, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/ --host 0.0.0.0 --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сравнение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f148979-269a-4be7-b108-f578e926aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CNN Results:\", cnn_results)\n",
    "print(\"LSTM Results:\", lstm_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itmo_dl_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
